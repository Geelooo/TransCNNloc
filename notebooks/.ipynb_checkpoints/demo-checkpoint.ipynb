{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook shows how to localize with PixLoc, visualize different quantities, and generate interactive animations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pformat\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import HTML\n",
    "\n",
    "from pixloc.settings import DATA_PATH, LOC_PATH\n",
    "from pixloc.localization import RetrievalLocalizer, SimpleTracker\n",
    "from pixloc.pixlib.geometry import Camera, Pose\n",
    "from pixloc.visualization.viz_2d import (\n",
    "    plot_images, plot_keypoints, add_text, features_to_RGB)\n",
    "from pixloc.visualization.animation import (\n",
    "    subsample_steps, VideoWriter, create_viz_dump, display_video)\n",
    "from pixloc.localization.localizer import lysLocalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/15/2022 10:36:33 pixloc.localization.model3d INFO] Reading COLMAP model /home/lys/Workplace/datasets/jiawei_905_outputs/colmap/sfm_superpoint+superglue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default paths:\n",
      "{'dataset': None,\n",
      " 'dumps': None,\n",
      " 'global_descriptors': PosixPath('jiawei_905_outputs/colmap/global-feats-netvlad.h5'),\n",
      " 'ground_truth': PosixPath('jiawei_905_outputs/colmap/sfm_superpoint+superglue'),\n",
      " 'hloc_logs': None,\n",
      " 'query_images': PosixPath('jiawei_905'),\n",
      " 'query_list': PosixPath('jiawei_905_outputs/query_list_with_intrinsics.txt'),\n",
      " 'reference_images': PosixPath('jiawei_905'),\n",
      " 'reference_sfm': PosixPath('jiawei_905_outputs/colmap/sfm_superpoint+superglue'),\n",
      " 'results': PosixPath('jiawei_905_outputs/results/pixloc_outputs.txt'),\n",
      " 'retrieval_pairs': PosixPath('jiawei_905_outputs/netvlad/pairs-query-netvlad.txt')}\n",
      "conf:\n",
      "{'experiment': 'pixloc_megadepth',\n",
      " 'features': {},\n",
      " 'optimizer': {'num_iters': 100, 'pad': 2},\n",
      " 'refinement': {'average_observations': False,\n",
      "                'do_pose_approximation': False,\n",
      "                'filter_covisibility': False,\n",
      "                'multiscale': [1],\n",
      "                'normalize_descriptors': True,\n",
      "                'num_dbs': 5,\n",
      "                'point_selection': 'all'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/15/2022 10:36:34 pixloc.utils.io INFO] Imported 1 images from query_list_with_intrinsics.txt\n",
      "[09/15/2022 10:36:34 pixloc.pixlib.utils.experiments INFO] Loading checkpoint checkpoint_best.tar\n"
     ]
    }
   ],
   "source": [
    "dataset = 'jiawei_905'\n",
    "\n",
    "if dataset == 'Aachen':\n",
    "    from pixloc.run_Aachen import default_paths, default_confs\n",
    "elif dataset == 'CMU':\n",
    "    from pixloc.run_CMU import default_paths, default_confs\n",
    "    default_paths = default_paths.interpolate(slice=21)\n",
    "else:\n",
    "    from pixloc.run_scripts import default_paths, default_confs\n",
    "    default_paths = default_paths.interpolate(scene=dataset)\n",
    "    \n",
    "print(f'default paths:\\n{pformat(default_paths.asdict())}')\n",
    "paths = default_paths.add_prefixes(DATA_PATH, LOC_PATH)\n",
    "\n",
    "conf = default_confs['from_retrieval']\n",
    "conf['refinement']['do_pose_approximation'] = False\n",
    "print(f'conf:\\n{pformat(conf)}')\n",
    "\n",
    "localizer = lysLocalizer(paths, conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localize one query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name_q = np.random.choice(list(localizer.queries))  # pick a random query\n",
    "# name_q = 'query/IMG_6585.JPG' # or select one for Aachen\n",
    "\n",
    "tracker = SimpleTracker(localizer.refiner)  # will hook & store the predictions\n",
    "cam_q = Camera.from_colmap(localizer.queries[name_q])\n",
    "ret = localizer.run_query(name_q, cam_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the CNN predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    ref_ids = ret['dbids'][:3]  # show 3 references at most\n",
    "    names_r = [localizer.model3d.dbs[i].name for i in ref_ids]\n",
    "    for name in names_r + [name_q]:\n",
    "        image, features, weights = tracker.dense[name][1]\n",
    "        features = features_to_RGB(features[1].numpy())[0]\n",
    "        plot_images([image, features, weights[1]], cmaps='turbo',\n",
    "                    titles=[name, 'features', 'confidence'], dpi=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the initial and final poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the first reference image for visualization\n",
    "ref_id = ref_ids[0]\n",
    "name_r = names_r[0]\n",
    "ref = localizer.model3d.dbs[ref_id]\n",
    "cam_r = Camera.from_colmap(localizer.model3d.cameras[ref.camera_id])\n",
    "T_w2r = Pose.from_colmap(ref)\n",
    "\n",
    "image_r = tracker.dense[name_r][1][0]\n",
    "image_q = tracker.dense[name_q][1][0]\n",
    "p3d = tracker.p3d\n",
    "T_w2q_init = tracker.T[0]\n",
    "T_w2q_final = tracker.T[-1]\n",
    "\n",
    "# Project the 3D points\n",
    "p2d_r, mask_r = cam_r.world2image(T_w2r * p3d)\n",
    "p2d_f, mask_f = cam_q.world2image(T_w2q_final * p3d)\n",
    "p2d_i, mask_i = cam_q.world2image(T_w2q_init * p3d)\n",
    "\n",
    "plot_images([image_r, image_q], dpi=75)\n",
    "plot_keypoints([p2d_r[mask_r], p2d_f[mask_f]], 'lime')\n",
    "plot_keypoints([None, p2d_i[mask_i]], 'red')\n",
    "add_text(0, 'reference')\n",
    "add_text(1, 'query')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the cost throughout the iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = tracker.costs\n",
    "fig, axes = plt.subplots(1, len(costs), figsize=(len(costs)*4.5, 4.5))\n",
    "for i, (ax, cost) in enumerate(zip(axes, costs)):\n",
    "    ax.plot(cost) if len(cost)>1 else ax.scatter(0., cost)\n",
    "    ax.set_title(f'({i}) Scale {i//3} Level {i%3}')\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_w2q_all = torch.stack(tracker.T)\n",
    "p2d_q_all, mask_q_all = cam_q.world2image(T_w2q_all * p3d)\n",
    "keep = subsample_steps(T_w2q_all, p2d_q_all, mask_q_all, cam_q.size.numpy())\n",
    "print(f'Keep {len(keep)}/{len(p2d_q_all)} optimization steps for visualization.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = 'pixloc.mp4'\n",
    "writer = VideoWriter('./tmp')\n",
    "for i in tqdm(keep):\n",
    "    plot_images([image_r, image_q], dpi=100, autoscale=False)\n",
    "    plot_keypoints([p2d_r[mask_r], p2d_q_all[-1][mask_q_all[-1]]], 'lime')\n",
    "    plot_keypoints([None, p2d_q_all[i][mask_q_all[i]]], 'red')\n",
    "    add_text(0, 'reference')\n",
    "    add_text(1, 'query')\n",
    "    writer.add_frame()\n",
    "writer.to_video(video, duration=4, crf=23)  # in seconds\n",
    "\n",
    "display_video(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize in 3D!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the path relative to the viewer folder\n",
    "viewer = Path('../viewer')\n",
    "assets = viewer / 'dumps/sample'\n",
    "\n",
    "# set the Y axis up for threejs\n",
    "tfm = np.eye(3)\n",
    "if dataset == 'CMU':\n",
    "    tfm = np.array([[1, 0, 0], [0, 0, 1], [0, -1, 0]])\n",
    "elif dataset == '7Scenes':\n",
    "    tfm = np.diag([1, -1, -1])\n",
    "\n",
    "# Write a json dump to viewer/assets/sample.json\n",
    "create_viz_dump(\n",
    "    assets, paths, cam_q, name_q, T_w2q_all[keep],\n",
    "    mask_q_all[keep], p2d_q_all[keep],\n",
    "    ref_ids, localizer.model3d, tracker.p3d_ids, tfm=tfm)\n",
    "\n",
    "with open(viewer / 'jupyter.html', 'r') as f:\n",
    "    html = f.read()\n",
    "html = html.replace('{__path__}', str(viewer))\n",
    "html = html.replace('{__assets__}', str(assets.parent))\n",
    "html = html.replace('{__height__}', '600px')\n",
    "HTML(html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
