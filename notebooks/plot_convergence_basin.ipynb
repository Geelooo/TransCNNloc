{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook plots the convergence basin for the multi-scale, multi-level features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pformat\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import kornia\n",
    "\n",
    "from pixloc.settings import DATA_PATH, LOC_PATH\n",
    "from pixloc.localization import *\n",
    "from pixloc.pixlib.geometry import Camera, Pose\n",
    "from pixloc.pixlib.geometry.interpolation import interpolate_tensor\n",
    "from pixloc.visualization.viz_2d import plot_images, plot_keypoints, add_text\n",
    "from pixloc.run_CMU import default_paths, default_confs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select one query and run the localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "default_paths = default_paths.interpolate(slice=21)\n",
    "print(f'default paths:\\n{pformat(default_paths.asdict())}')\n",
    "paths = default_paths.add_prefixes(DATA_PATH/'CMU', LOC_PATH/'CMU')\n",
    "\n",
    "conf = default_confs['from_retrieval']\n",
    "conf['refinement']['multiscale'] = [4, 2, 1]\n",
    "print(f'conf:\\n{pformat(conf)}')\n",
    "localizer = LysLocalizer(paths, conf)\n",
    "\n",
    "name_q = np.random.RandomState(17).choice(list(localizer.queries), 10)[-1]\n",
    "tracker = SimpleTracker(localizer.refiner)  # will hook & store the predictions\n",
    "cam_q = Camera.from_colmap(localizer.queries[name_q])\n",
    "ret = localizer.run_query(name_q, cam_q)\n",
    "\n",
    "ref_ids = ret['dbids'][:3]  # show 3 references at most\n",
    "names_r = [localizer.model3d.dbs[i].name for i in ref_ids]\n",
    "for name in names_r + [name_q]:\n",
    "    image, _, weights = tracker.dense[name][1]\n",
    "    plot_images([image, weights[1]], cmaps='turbo',\n",
    "                titles=[name, 'confidence'], dpi=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the reprojections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the first reference image for visualization\n",
    "ref_id = ref_ids[0]\n",
    "name_r = names_r[0]\n",
    "ref = localizer.model3d.dbs[ref_id]\n",
    "cam_r = Camera.from_colmap(localizer.model3d.cameras[ref.camera_id])\n",
    "T_w2r = Pose.from_colmap(ref)\n",
    "\n",
    "image_r = tracker.dense[name_r][1][0]\n",
    "image_q = tracker.dense[name_q][1][0]\n",
    "p3d = tracker.p3d\n",
    "T_w2q_init = tracker.T[0]\n",
    "T_w2q_final = tracker.T[-1]\n",
    "\n",
    "# Project the 3D points\n",
    "p2d_r, mask_r = cam_r.world2image(T_w2r * p3d)\n",
    "p2d_f, mask_f = cam_q.world2image(T_w2q_final * p3d)\n",
    "p2d_i, mask_i = cam_q.world2image(T_w2q_init * p3d)\n",
    "\n",
    "plot_images([image_r, image_q], dpi=75)\n",
    "plot_keypoints([p2d_r[mask_r], p2d_f[mask_f]], 'lime')\n",
    "plot_keypoints([None, p2d_i[mask_i]], 'red')\n",
    "add_text(0, 'reference')\n",
    "add_text(1, 'query')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select a point for which we want to compute the convergence basin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just click once and run the next cell to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plot_images([image_q])\n",
    "plot_keypoints([p2d_f[mask_f]], ps=10)\n",
    "fig, ax = plt.gcf(), plt.gca()\n",
    "sc = ax.collections[0]\n",
    "sc.set_picker(True)\n",
    "picks = []\n",
    "\n",
    "def onpick(event):\n",
    "    if event.artist == sc:\n",
    "        picks.append(event)\n",
    "        print(event)\n",
    "\n",
    "fig.canvas.mpl_connect(\"pick_event\", onpick)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# get the keypoint index\n",
    "idx = picks[-1].ind[0]\n",
    "idx = np.where(mask_f.numpy())[0][idx]\n",
    "print(idx)\n",
    "\n",
    "plot_images([image_q, image_r], dpi=100)\n",
    "plot_keypoints([p2d_f[idx][None], p2d_r[idx][None]], 'red', ps=60)\n",
    "add_text(0, 'query')\n",
    "add_text(1, 'reference')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the dense basin at each scale and level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_basin_2(grad, pt, size=0, init=None):\n",
    "    filters = torch.arange(9).reshape(3, 3)[None] == torch.arange(9)[:, None, None]\n",
    "    filters = torch.stack([filters[i] for i in [5, 8, 7, 6, 3, 0, 1, 2]]).float()\n",
    "    gdangle = torch.atan2(-grad[:, :, 1], -grad[:, :, 0])\n",
    "    binned = s = (gdangle*180/np.pi % 360) / 45\n",
    "    prob = torch.clamp(1 - torch.abs(binned - torch.arange(9)[:, None, None]), min=0)\n",
    "    prob[0] = torch.max(prob[0], prob[-1])\n",
    "    prob = prob[:-1]\n",
    "\n",
    "    if init is None:\n",
    "        init = torch.zeros_like(gdangle)\n",
    "        x, y = np.round(pt).astype(int)\n",
    "        init[y-size:y+size+1, x-size:x+size+1] = 1.\n",
    "\n",
    "    basin = init\n",
    "    for it in range(1000):\n",
    "        conved = torch.nn.functional.conv2d(basin[None, None].float(), filters[:, None], padding=1)[0]\n",
    "        updated = torch.max(basin, torch.sum(prob*conved, 0))\n",
    "        diff = (updated - basin).abs()\n",
    "        basin = updated\n",
    "        if diff.max() < 1e-2:\n",
    "            break\n",
    "    if diff.max() > 1e-2:\n",
    "        print('did not converge')\n",
    "    return basin\n",
    "\n",
    "interp = lambda x, y: torch.nn.functional.interpolate(\n",
    "    x[None], y, mode='bilinear', align_corners=False)[0]\n",
    "\n",
    "grads = []\n",
    "basins = []\n",
    "scale_level = []\n",
    "for resize in [1, 2, 4]:\n",
    "    for i, level in enumerate([0, 1, 2]):\n",
    "        Fq = tracker.dense[name_q][resize][1][level]\n",
    "        Fr = tracker.dense[name_r][resize][1][level]\n",
    "        scale = localizer.refiner.feature_extractor.model.scales[level]*resize\n",
    "        p_q_scaled = (p2d_f[idx][None]+.5)/scale-.5\n",
    "        p_r_scaled = (p2d_r[idx][None]+.5)/scale-.5\n",
    "        p_q_scaled = p_q_scaled.numpy()\n",
    "        if len(grads) == 0:\n",
    "            p_q_scaled0 = p_q_scaled\n",
    "\n",
    "        Fri, _, _ = interpolate_tensor(Fr, p_r_scaled.float())\n",
    "\n",
    "        res = Fq - Fri[0, :, None, None]\n",
    "        Fq_diff = kornia.filters.spatial_gradient(Fq[None], 'diff')[0]\n",
    "        grad = torch.einsum('dnhw,dhw->hwn', Fq_diff, res)\n",
    "        basin = compute_basin_2(grad, p_q_scaled[0], 1)\n",
    "        if len(grads) != 0:\n",
    "            grad = interp(grad.permute(2, 0, 1), grads[0].shape[:2]).permute(1, 2, 0)\n",
    "            basin = interp(basin[None], grads[0].shape[:2])[0]\n",
    "        grads.append(grad)\n",
    "        basins.append(basin)\n",
    "        scale_level.append((resize, level))\n",
    "\n",
    "p_q_scaled = p_q_scaled0\n",
    "plot_images(basins, titles=scale_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain the basins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- accumulated basins as we add more levels and scales\n",
    "- binarized values\n",
    "- contribution of each level and scale\n",
    "\n",
    "columns:\n",
    "- last column = low resolution coarse level\n",
    "- first column = high resolution fine level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basins_chained = []\n",
    "contributions = []\n",
    "init = None\n",
    "for grad in grads:\n",
    "    basin = compute_basin_2(grad, p_q_scaled[0], 2, init=init)\n",
    "    basins_chained.append(basin)\n",
    "    basin = (basin>0.5).float()\n",
    "    init = basin\n",
    "    if len(contributions) == 0:\n",
    "        contributions.append(basin)\n",
    "    else:\n",
    "        contributions.append((1-torch.stack(contributions, 0).sum(0))*basin)\n",
    "    \n",
    "plot_images(basins_chained, titles=scale_level)\n",
    "plot_images([b>0 for b in basins_chained], titles=scale_level)\n",
    "plot_images(contributions, titles=scale_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretty plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the pixelwise gradient angle at the corresponding levels\n",
    "grads_dir = torch.nn.functional.normalize(torch.stack(grads, 0), dim=3)\n",
    "grad = torch.sum(grads_dir * torch.stack(contributions, 0)[..., None], 0)\n",
    "angle = torch.atan2(grad[:, :, 1], grad[:, :, 0])\n",
    "\n",
    "# plot as HSV field\n",
    "p = angle/np.pi/2 + 0.5\n",
    "hsv_field = cm.hsv(p.numpy())[:,:, :3]\n",
    "plot_images([hsv_field*basins_chained[-1].numpy()[:, :, None]], dpi=100)\n",
    "# plot_images([basins_chained[-1].numpy()[:, :, None]], dpi=100)\n",
    "\n",
    "# add image overlay background\n",
    "[a.images[0].set(alpha=0.5) for a in plt.gcf().axes];\n",
    "[a.imshow(image_q, extent=a.images[0]._extent, zorder=-2) for a in plt.gcf().axes];\n",
    "\n",
    "# draw the arrows\n",
    "basin = basins_chained[-1]  # final basin for filtering\n",
    "flow = -torch.nn.functional.normalize(grad, dim=-1)\n",
    "skip = 32\n",
    "offset = skip//2\n",
    "flow = flow[offset::skip, offset::skip]\n",
    "U, V = flow[:, :, 0], flow[:, :, 1]\n",
    "h, w = grad.shape[:2]\n",
    "Y, X = np.mgrid[offset:h:skip, offset:w:skip]\n",
    "qbasin = basin[offset::skip, offset::skip]>.5\n",
    "X, Y, U, V = X[qbasin], Y[qbasin], U[qbasin], V[qbasin]\n",
    "if level != 0:\n",
    "    plt.quiver(X, Y, U, V, scale=.07, scale_units='xy', angles='xy')\n",
    "plot_keypoints([p_q_scaled], 'red', ps=60)\n",
    "\n",
    "# some stats\n",
    "h, w = basin.shape\n",
    "dists = np.linalg.norm(np.mgrid[:h, :w] - p_q_scaled[0][::-1][:, None, None], axis=0)\n",
    "top10 = np.percentile(dists[basin>0.5], 90)\n",
    "\n",
    "text = [\n",
    "    'Convergence basin\\nin query',\n",
    "    f'10% at >{top10:.0f}px'\n",
    "]\n",
    "add_text(0, '\\n'.join(text), lwidth=0)\n",
    "add_text(0, name_q, pos=(0.01, 0.02), fs=5, lwidth=0)\n",
    "\n",
    "plot_images([cm.hsv(np.arctan2(*np.mgrid[-20:20, -20:20])/np.pi/2 + 0.5)], dpi=50)\n",
    "add_text(0, 'colormap', color='k', lwidth=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
